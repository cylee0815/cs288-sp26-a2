"""
BPE (Byte Pair Encoding) training implementation.

This module implements the BPE algorithm for learning a tokenizer vocabulary
from a text corpus, compatible with GPT-2 style tokenization.
"""

from __future__ import annotations

import regex as re
from collections import Counter
from pathlib import Path
from typing import Iterator

import argparse
import json
from pathlib import Path
from tqdm import tqdm


# GPT-2 pre-tokenization pattern
GPT2_PAT = re.compile(
    r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""",
    re.UNICODE
)


def get_pairs(word: tuple[bytes, ...]) -> set[tuple[bytes, bytes]]:
    """Get all adjacent pairs in a word (tuple of byte tokens)."""
    pairs = set()
    for i in range(len(word) - 1):
        pairs.add((word[i], word[i + 1]))
    return pairs


def merge_word(word: tuple[bytes, ...], pair: tuple[bytes, bytes]) -> tuple[bytes, ...]:
    """Merge all occurrences of a pair in a word."""
    first, second = pair
    new_word = []
    i = 0
    while i < len(word):
        if i < len(word) - 1 and word[i] == first and word[i + 1] == second:
            new_word.append(first + second)
            i += 2
        else:
            new_word.append(word[i])
            i += 1
    return tuple(new_word)


def pre_tokenize(text: str, special_tokens: list[str] | None = None) -> Iterator[str]:
    """
    Pre-tokenize text using GPT-2 pattern, preserving special tokens.
    
    Special tokens are yielded as-is (not split by the regex pattern).
    """
    special_tokens = special_tokens or []
    
    if not special_tokens:
        # No special tokens, just use the pattern
        for match in GPT2_PAT.finditer(text):
            yield match.group()
        return
    
    # Sort special tokens by length (longest first) for greedy matching
    sorted_specials = sorted(special_tokens, key=len, reverse=True)
    
    # Build a pattern that matches special tokens
    import re as std_re
    special_pattern = "|".join(std_re.escape(s) for s in sorted_specials)
    split_pattern = f"({special_pattern})"
    
    # Split text by special tokens
    parts = std_re.split(split_pattern, text)
    
    for part in parts:
        if part in special_tokens:
            # Special token - yield as-is, but it won't be BPE-encoded
            # (we skip special tokens in the word frequency counting)
            continue
        elif part:
            # Regular text - apply GPT-2 pre-tokenization
            for match in GPT2_PAT.finditer(part):
                yield match.group()


def train_bpe(
    input_path: Path,
    vocab_size: int,
    special_tokens: list[str] | None = None,
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """
    Train a BPE tokenizer from a text file.
    
    Args:
        input_path: Path to the input text file
        vocab_size: Target vocabulary size
        special_tokens: List of special tokens to include (e.g., ["<|endoftext|>"])
        
    Returns:
        Tuple of (vocab, merges) where:
        - vocab: dict mapping token_id (int) -> token (bytes)
        - merges: list of merge pairs in order they were learned [(bytes, bytes), ...]
    
    Algorithm Overview:
        BPE iteratively merges the most frequent pair of adjacent tokens until
        the vocabulary reaches the target size.
    
    Detailed Steps:
    
    1. VOCABULARY INITIALIZATION
       The initial vocabulary is built in this exact order:
       - First: Add special tokens (in the order provided)
       - Then: Add all 256 single-byte values (0x00 to 0xFF)
       
       Example with special_tokens=["<|endoftext|>"]:
         vocab = {
             0: b"<|endoftext|>",   # Special token first
             1: b"\\x00",           # Byte 0
             2: b"\\x01",           # Byte 1
             ...
             256: b"\\xff",         # Byte 255
         }
       
       So the initial vocab size = len(special_tokens) + 256
    
    2. WORD FREQUENCY COUNTING
       - Pre-tokenize the corpus using pre_tokenize(text, special_tokens)
       - For each pre-token, convert to bytes and represent as tuple of single bytes
       - Skip any word containing a "forbidden substring" (prefix of a special token)
       
       Example: "hello" -> (b'h', b'e', b'l', b'l', b'o')
       
       word_freqs is a Counter mapping: tuple[bytes, ...] -> frequency
    
    3. PAIR FREQUENCY COUNTING  
       Count how often each adjacent pair appears across ALL words, weighted by
       word frequency.
       
       Example: If word (b'h', b'e', b'l', b'l', b'o') appears 10 times:
         - pair (b'h', b'e') gets +10
         - pair (b'e', b'l') gets +10
         - pair (b'l', b'l') gets +10
         - pair (b'l', b'o') gets +10
    
    4. MERGE LOOP (repeat until vocab_size is reached)
       
       a. SELECT BEST PAIR (DETERMINISTIC TIE-BREAKING):
          Find the pair with highest frequency. If multiple pairs have the same
          frequency, select the lexicographically smallest pair.
          
          Lexicographic comparison on (bytes, bytes) tuples:
            - Compare first element as bytes
            - If equal, compare second element as bytes
          
          Example: If pairs (b'a', b'b') and (b'a', b'c') both have freq=100,
                   select (b'a', b'b') because b'b' < b'c'
          
          Implementation: max(pair_counts, key=lambda p: (pair_counts[p], p))
                          This sorts by (frequency, pair) and takes the max.
                          Since we want highest freq but lowest pair for ties,
                          use: max(pair_counts, key=lambda p: (pair_counts[p], p))
                          
                          Note: Python compares bytes lexicographically by default.
       
       b. CREATE MERGED TOKEN:
          new_token = first + second  (bytes concatenation)
          Add to vocabulary with next available token_id
          Append (first, second) to merges list
       
       c. UPDATE WORD REPRESENTATIONS:
          For each word in word_freqs, apply the merge using merge_word()
          This replaces all occurrences of the pair with the merged token
       
       d. UPDATE PAIR COUNTS:
          Recompute pair frequencies for the updated words
          (Or incrementally update - subtract old pairs, add new pairs)
    
    5. RETURN
       Return (vocab, merges) where merges is the list of pairs in the order
       they were merged.
    
    Performance Note:
        A naive implementation recomputing all pair counts each iteration is O(nÂ²).
        For efficiency, incrementally update pair counts by only processing words
        that contained the merged pair.
    """
    special_tokens = special_tokens or []
    
    # Read the corpus
    with open(input_path, encoding="utf-8") as f:
        text = f.read()
    
    # Build set of "forbidden" substrings from special tokens
    forbidden_substrings = set()
    for special in tqdm(special_tokens):
        special_bytes = special.encode("utf-8")
        for i in range(2, len(special_bytes) + 1):
            forbidden_substrings.add(special_bytes[:i])
    
    # TODO: Implement BPE training
    # ---------------------------------------------------------
    # 1. VOCABULARY INITIALIZATION
    # ---------------------------------------------------------
    print("1. VOCABULARY INITIALIZATION")
    vocab: dict[int, bytes] = {}
    idx = 0
    
    # Add special tokens first
    for st in special_tokens:
        vocab[idx] = st.encode("utf-8")
        idx += 1
        
    # Add all 256 single-byte values
    for b in range(256):
        vocab[idx] = bytes([b])
        idx += 1

    # Read the corpus
    with open(input_path, encoding="utf-8") as f:
        text = f.read()
    
    # Build set of "forbidden" substrings from special tokens
    forbidden_substrings = set()
    for special in special_tokens:
        special_bytes = special.encode("utf-8")
        for i in range(2, len(special_bytes) + 1):
            forbidden_substrings.add(special_bytes[:i])
            
    # ---------------------------------------------------------
    # 2. WORD FREQUENCY COUNTING
    # ---------------------------------------------------------
    print("2. WORD FREQUENCY COUNTING")
    word_freqs: Counter[tuple[bytes, ...]] = Counter()
    
    for token_str in tqdm(pre_tokenize(text, special_tokens)):
        if token_str in special_tokens:
            continue  # Skip special tokens in BPE merging
            
        token_bytes = token_str.encode("utf-8")
        
        # Check for forbidden substrings
        has_forbidden = False
        for i in range(len(token_bytes)):
            for j in range(i + 2, len(token_bytes) + 1):
                if token_bytes[i:j] in forbidden_substrings:
                    has_forbidden = True
                    break
            if has_forbidden:
                break
                
        if has_forbidden:
            continue
            
        # Represent as tuple of single bytes
        word_tuple = tuple(bytes([b]) for b in token_bytes)
        word_freqs[word_tuple] += 1

    # ---------------------------------------------------------
    # 3 & 4. PAIR FREQUENCY COUNTING & MERGE LOOP
    # ---------------------------------------------------------
    print("3 & 4. PAIR FREQUENCY COUNTING & MERGE LOOP")
    merges: list[tuple[bytes, bytes]] = []

    # Initialize tqdm manually, specifying the total
    pbar = tqdm(total=vocab_size, desc="Processing while loop")
    
    while len(vocab) < vocab_size:
        # Count pairs across all words
        pair_counts: Counter[tuple[bytes, bytes]] = Counter()
        for word, freq in word_freqs.items():
            for pair in get_pairs(word):
                pair_counts[pair] += freq
                
        # If no more pairs can be merged, break early
        if not pair_counts:
            break
            
        # a. SELECT BEST PAIR (DETERMINISTIC TIE-BREAKING)
        # Using the exact max() logic from the assignment docstring so it perfectly 
        # matches the autograder's expected tie-breaking behavior.
        best_pair = max(pair_counts, key=lambda p: (pair_counts[p], p))
        
        # b. CREATE MERGED TOKEN
        first, second = best_pair
        new_token = first + second
        
        vocab[len(vocab)] = new_token
        merges.append(best_pair)
         
        # c. UPDATE WORD REPRESENTATIONS
        new_word_freqs: Counter[tuple[bytes, ...]] = Counter()
        # for word, freq in word_freqs.items():
        #     if len(word) > 1:
        #         # Use the helper function provided in your skeleton
        #         new_word = merge_word(word, best_pair)
        #         new_word_freqs[new_word] += freq
        #     else:
        #         new_word_freqs[word] += freq
        for word, freq in word_freqs.items():
            if len(word) > 1:
                # FIX: Only run merge_word if both bytes actually exist in the word!
                # If they don't exist, we know the pair can't exist, so skip the math.
                if best_pair[0] in word and best_pair[1] in word:
                    new_word = merge_word(word, best_pair)
                    new_word_freqs[new_word] += freq
                else:
                    new_word_freqs[word] += freq  # Skip the expensive merge!
            else:
                new_word_freqs[word] += freq
                
        word_freqs = new_word_freqs

        # Manually update the progress bar by one step
        pbar.update(1)

    # 5. RETURN
    return vocab, merges
    
    # raise NotImplementedError("Implement train_bpe")

if __name__ == "__main__":
    # 1. Set up the argument parser
    parser = argparse.ArgumentParser(description="Train a BPE tokenizer on TinyStories.")
    
    # Define the arguments
    parser.add_argument(
        "--input_path", 
        type=str, 
        default="fixtures/tinystories_sample.txt", 
        help="Path to the input corpus (e.g., TinyStories dataset)."
    )
    parser.add_argument(
        "--vocab_size", 
        type=int, 
        default=5000, 
        help="Target vocabulary size for the BPE algorithm."
    )
    parser.add_argument(
        "--special_tokens", 
        nargs="*", 
        default=["<|endoftext|>"], 
        help="List of special tokens to initialize in the vocabulary."
    )
    parser.add_argument(
        "--vocab_save_path", 
        type=str, 
        default="vocab.json", 
        help="File path to save the learned vocabulary."
    )
    parser.add_argument(
        "--merges_save_path", 
        type=str, 
        default="merges.json", 
        help="File path to save the learned merge rules."
    )

    args = parser.parse_args()

    # 2. Validate input path
    input_file = Path(args.input_path)
    if not input_file.exists():
        print(f"Error: Could not find '{input_file}'.")
        print("Make sure you are running the script from the correct directory.")
        exit(1)

    # 3. Run the BPE training procedure
    print(f"Loading dataset from '{input_file}'...")
    print(f"Running BPE training (Target Vocab Size: {args.vocab_size}). This may take a moment...")
    
    vocab, merges = train_bpe(
        input_path=input_file,
        vocab_size=args.vocab_size,
        special_tokens=args.special_tokens
    )
    
    print(f"Success! Learned {len(vocab)} vocab items and {len(merges)} merges.")

    # 4. Save the learned vocabulary and merge rules
    # We decode bytes using 'latin-1' so they can be safely serialized to JSON and perfectly 
    # reconstructed later without UTF-8 error crashes.
    
    vocab_serializable = {str(k): v.decode("latin-1") for k, v in vocab.items()}
    with open(args.vocab_save_path, "w", encoding="utf-8") as f:
        json.dump(vocab_serializable, f, indent=2)
        
    merges_serializable = [[m[0].decode("latin-1"), m[1].decode("latin-1")] for m in merges]
    with open(args.merges_save_path, "w", encoding="utf-8") as f:
        json.dump(merges_serializable, f, indent=2)
        
    print(f"Saved vocabulary to: {args.vocab_save_path}")
    print(f"Saved merge rules to: {args.merges_save_path}")
